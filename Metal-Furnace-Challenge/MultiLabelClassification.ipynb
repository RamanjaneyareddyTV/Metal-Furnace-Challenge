{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metal Furnace Challenge : Weekend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manufacturing of any alloy is not a simple process. Many complicated factors are involved in the making of a perfect alloy, from the temperature at which various metals are melted to the presence of impurities to the cooling temperature set to cool down the alloy. Very minor changes in any of these factors can affect the quality or grade of the alloy produced.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given are 28 distinguishing factors in the manufacturing of an alloy, your objective as a data scientist is to build a Machine Learning model that can predict the grade of the product using these factors.\n",
    "\n",
    "You are provided with 28 anonymized factors (f0 to f27) that influence the making of a perfect alloy that is to be used for various applications based on the grade/quality of the obtained product.\n",
    "\n",
    "**Data Description**  \n",
    "The unzipped folder will have the following files.\n",
    "\n",
    "- Train.csv – 620 observations.\n",
    "- Test.csv – 266 observations.\n",
    "- Sample Submission – Sample format for the submission.\n",
    "- Target Variable: grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Print all rows and columns. Dont hide any\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split,cross_val_predict\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Train.csv\")\n",
    "test = pd.read_csv(\"Test.csv\")\n",
    "sample = pd.read_excel(\"Sample_Submission.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((620, 29), (266, 28))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.848564</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>2.329398</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.548623</td>\n",
       "      <td>0.472101</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>0.443257</td>\n",
       "      <td>-0.406121</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>3.727218</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>3.032397</td>\n",
       "      <td>-2.442599</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.548623</td>\n",
       "      <td>0.472101</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>-0.232546</td>\n",
       "      <td>-0.406366</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.848564</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>2.329398</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.548623</td>\n",
       "      <td>0.472101</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>1.459782</td>\n",
       "      <td>1.221876</td>\n",
       "      <td>1.877777</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511733</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>-1.999287</td>\n",
       "      <td>-2.118189</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>-3.237512</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>-0.008030</td>\n",
       "      <td>-0.406366</td>\n",
       "      <td>1.504523</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.526055</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>-1.999287</td>\n",
       "      <td>-2.118189</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>-0.573268</td>\n",
       "      <td>-1.164793</td>\n",
       "      <td>1.877777</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>0.102129</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0       f1        f2        f3        f4        f5        f6  \\\n",
       "0  1.848564 -0.26425 -0.461423  0.409400  1.305455  2.329398  0.370965   \n",
       "1 -0.825098 -0.26425  3.032397 -2.442599  1.305455 -0.276144  0.370965   \n",
       "2  1.848564 -0.26425 -0.461423  0.409400  1.305455  2.329398  0.370965   \n",
       "3  0.511733 -0.26425 -0.461423  0.409400 -0.525726 -0.276144  0.370965   \n",
       "4 -0.825098 -0.26425 -0.461423  0.409400 -0.525726 -0.276144  0.370965   \n",
       "\n",
       "         f7        f8   f9       f10       f11       f12       f13       f14  \\\n",
       "0  0.090167  0.107958  0.0  0.395874  0.308879  0.548623  0.472101  0.172917   \n",
       "1  0.090167  0.107958  0.0  0.395874  0.308879  0.548623  0.472101  0.172917   \n",
       "2  0.090167  0.107958  0.0  0.395874  0.308879  0.548623  0.472101  0.172917   \n",
       "3  0.090167  0.107958  0.0  0.395874  0.308879 -1.999287 -2.118189  0.172917   \n",
       "4  0.090167  0.107958  0.0 -2.526055  0.308879 -1.999287 -2.118189  0.172917   \n",
       "\n",
       "        f15       f16       f17       f18       f19       f20       f21  \\\n",
       "0  0.098853  0.308879  0.040193  0.182574  0.085505  0.233285 -1.080663   \n",
       "1  0.098853  0.308879  0.040193  0.182574  0.085505  0.233285 -1.080663   \n",
       "2  0.098853  0.308879  0.040193  0.182574  0.085505  0.233285  0.925358   \n",
       "3  0.098853 -3.237512  0.040193  0.182574  0.085505  0.233285  0.925358   \n",
       "4  0.098853  0.308879  0.040193  0.182574  0.085505  0.233285  0.925358   \n",
       "\n",
       "        f22       f23       f24       f25       f26       f27  grade  \n",
       "0  0.443257 -0.406121 -0.687687  0.271886  3.727218  0.102129      2  \n",
       "1 -0.232546 -0.406366 -0.687687  0.271886 -0.232472  0.102129      4  \n",
       "2  1.459782  1.221876  1.877777  0.271886 -0.232472  0.102129      2  \n",
       "3 -0.008030 -0.406366  1.504523  0.271886 -0.232472  0.102129      2  \n",
       "4 -0.573268 -1.164793  1.877777  0.271886 -0.232472  0.102129      2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.837812</td>\n",
       "      <td>-0.273636</td>\n",
       "      <td>1.276580</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>-0.585142</td>\n",
       "      <td>-0.24287</td>\n",
       "      <td>0.349804</td>\n",
       "      <td>0.12356</td>\n",
       "      <td>0.166795</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.445195</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>-2.139737</td>\n",
       "      <td>-2.527625</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.285133</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.197642</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.886135</td>\n",
       "      <td>-0.568935</td>\n",
       "      <td>1.100428</td>\n",
       "      <td>-0.244589</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>-0.217109</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.078087</td>\n",
       "      <td>-0.273636</td>\n",
       "      <td>-0.496119</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>-2.438092</td>\n",
       "      <td>-0.24287</td>\n",
       "      <td>0.349804</td>\n",
       "      <td>0.12356</td>\n",
       "      <td>0.166795</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.445195</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.513736</td>\n",
       "      <td>0.395628</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.285133</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>-5.059644</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.886135</td>\n",
       "      <td>0.504299</td>\n",
       "      <td>-0.434268</td>\n",
       "      <td>-0.244040</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>-0.217109</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.837812</td>\n",
       "      <td>-0.273636</td>\n",
       "      <td>1.276580</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>-0.585142</td>\n",
       "      <td>-0.24287</td>\n",
       "      <td>0.349804</td>\n",
       "      <td>0.12356</td>\n",
       "      <td>0.166795</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.445195</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.513736</td>\n",
       "      <td>0.395628</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.285133</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.197642</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>-1.128496</td>\n",
       "      <td>-0.568935</td>\n",
       "      <td>-0.434268</td>\n",
       "      <td>-0.662763</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>-0.217109</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.837812</td>\n",
       "      <td>-0.273636</td>\n",
       "      <td>-0.496119</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>1.267808</td>\n",
       "      <td>-0.24287</td>\n",
       "      <td>-2.858743</td>\n",
       "      <td>0.12356</td>\n",
       "      <td>0.166795</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.445195</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.513736</td>\n",
       "      <td>0.395628</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.285133</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>-5.059644</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>-1.128496</td>\n",
       "      <td>-0.449819</td>\n",
       "      <td>-1.918647</td>\n",
       "      <td>-0.662763</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>-0.217109</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.837812</td>\n",
       "      <td>-0.273636</td>\n",
       "      <td>-0.496119</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>-0.585142</td>\n",
       "      <td>-0.24287</td>\n",
       "      <td>-2.858743</td>\n",
       "      <td>0.12356</td>\n",
       "      <td>0.166795</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.445195</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.513736</td>\n",
       "      <td>0.395628</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.285133</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.197642</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>-1.128496</td>\n",
       "      <td>-0.568935</td>\n",
       "      <td>-0.434268</td>\n",
       "      <td>-0.662763</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>-0.217109</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0        f1        f2        f3        f4       f5        f6  \\\n",
       "0 -0.837812 -0.273636  1.276580  0.463262 -0.585142 -0.24287  0.349804   \n",
       "1  2.078087 -0.273636 -0.496119  0.463262 -2.438092 -0.24287  0.349804   \n",
       "2 -0.837812 -0.273636  1.276580  0.463262 -0.585142 -0.24287  0.349804   \n",
       "3 -0.837812 -0.273636 -0.496119  0.463262  1.267808 -0.24287 -2.858743   \n",
       "4 -0.837812 -0.273636 -0.496119  0.463262 -0.585142 -0.24287 -2.858743   \n",
       "\n",
       "        f7        f8       f9       f10      f11       f12       f13      f14  \\\n",
       "0  0.12356  0.166795  0.06143  0.445195  0.27735 -2.139737 -2.527625  0.17609   \n",
       "1  0.12356  0.166795  0.06143  0.445195  0.27735  0.513736  0.395628  0.17609   \n",
       "2  0.12356  0.166795  0.06143  0.445195  0.27735  0.513736  0.395628  0.17609   \n",
       "3  0.12356  0.166795  0.06143  0.445195  0.27735  0.513736  0.395628  0.17609   \n",
       "4  0.12356  0.166795  0.06143  0.445195  0.27735  0.513736  0.395628  0.17609   \n",
       "\n",
       "       f15       f16      f17       f18      f19      f20       f21       f22  \\\n",
       "0  0.06143  0.285133  0.06143  0.197642  0.06143  0.27735  0.886135 -0.568935   \n",
       "1  0.06143  0.285133  0.06143 -5.059644  0.06143  0.27735  0.886135  0.504299   \n",
       "2  0.06143  0.285133  0.06143  0.197642  0.06143  0.27735 -1.128496 -0.568935   \n",
       "3  0.06143  0.285133  0.06143 -5.059644  0.06143  0.27735 -1.128496 -0.449819   \n",
       "4  0.06143  0.285133  0.06143  0.197642  0.06143  0.27735 -1.128496 -0.568935   \n",
       "\n",
       "        f23       f24       f25       f26       f27  \n",
       "0  1.100428 -0.244589  0.229718 -0.217109  0.087039  \n",
       "1 -0.434268 -0.244040  0.229718 -0.217109  0.087039  \n",
       "2 -0.434268 -0.662763  0.229718 -0.217109  0.087039  \n",
       "3 -1.918647 -0.662763  0.229718 -0.217109  0.087039  \n",
       "4 -0.434268 -0.662763  0.229718 -0.217109  0.087039  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    472\n",
       "1     68\n",
       "3     47\n",
       "4     27\n",
       "0      6\n",
       "Name: grade, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f0        8\n",
       "f1       10\n",
       "f2        7\n",
       "f3        2\n",
       "f4        3\n",
       "f5        8\n",
       "f6        2\n",
       "f7        2\n",
       "f8        3\n",
       "f9        1\n",
       "f10       2\n",
       "f11       2\n",
       "f12       3\n",
       "f13       2\n",
       "f14       2\n",
       "f15       2\n",
       "f16       2\n",
       "f17       2\n",
       "f18       2\n",
       "f19       4\n",
       "f20       2\n",
       "f21       2\n",
       "f22      49\n",
       "f23      63\n",
       "f24      24\n",
       "f25       3\n",
       "f26       3\n",
       "f27       3\n",
       "grade     5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f0      8\n",
       "f1      7\n",
       "f2      6\n",
       "f3      2\n",
       "f4      3\n",
       "f5      6\n",
       "f6      2\n",
       "f7      2\n",
       "f8      3\n",
       "f9      2\n",
       "f10     2\n",
       "f11     2\n",
       "f12     3\n",
       "f13     2\n",
       "f14     2\n",
       "f15     2\n",
       "f16     2\n",
       "f17     2\n",
       "f18     2\n",
       "f19     2\n",
       "f20     2\n",
       "f21     2\n",
       "f22    37\n",
       "f23    41\n",
       "f24    14\n",
       "f25     3\n",
       "f26     3\n",
       "f27     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.machinehack.com/wp-content/uploads/2020/02/MULTI_CLASS_LOGLOSS_FORM.png\" height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y,y0):\n",
    "    return log_loss(y,y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing the f9 feature as the train data contains only 1 unique value and f9 feature did not have feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(set(train.columns)-set(['grade','f9']))\n",
    "target = 'grade'\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since, the test data was only 30% on public leaderboard, it was diffcuilt to match the local validation score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid(model,train,features,target,cv=3):\n",
    "    results = cross_val_predict(model, train[features], train[target], method=\"predict_proba\",cv=cv)\n",
    "    return metric(train[target],results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseliner Models are checked to find good performing basliner models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier 0.21265458917864544\n",
      "[03:28:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:28:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier 0.14978414054730232\n",
      "GradientBoostingClassifier 0.209728401447998\n",
      "LogisticRegression 0.3636447642289338\n",
      "RandomForestClassifier 0.2493366278070803\n",
      "AdaBoostClassifier 1.3349991672743202\n"
     ]
    }
   ],
   "source": [
    "models = [lgb.LGBMClassifier(), xgb.XGBClassifier(), GradientBoostingClassifier(), LogisticRegression(), \n",
    "              RandomForestClassifier(), AdaBoostClassifier()\n",
    "             ]\n",
    "\n",
    "for i in models:\n",
    "    model = i\n",
    "    error = cross_valid(model,train,features,target,cv=10)\n",
    "    print(str(model).split(\"(\")[0], error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(train, features, target, plot=True):\n",
    "    evals_result = {}\n",
    "    trainX, validX, trainY, validY = train_test_split(train[features], train[target], test_size=0.2, random_state=13)\n",
    "    print(\"XGB Model\")\n",
    "    \n",
    "    dtrain = xgb.DMatrix(trainX, label=trainY)\n",
    "    dvalid = xgb.DMatrix(validX, label=validY)\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "    \n",
    "    MAX_ROUNDS=2000\n",
    "    early_stopping_rounds=100\n",
    "    params = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'learning_rate': 0.01,\n",
    "        'num_round': MAX_ROUNDS,\n",
    "        'max_depth': 8,\n",
    "        'seed': 25,\n",
    "        'nthread': -1,\n",
    "        'num_class':5\n",
    "    }\n",
    "    \n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        evals=watchlist,\n",
    "        num_boost_round=MAX_ROUNDS,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=50\n",
    "        #feval=metric_xgb\n",
    "    \n",
    "    )\n",
    "    \n",
    "    print(\"Best Iteration :: {} \\n\".format(model.best_iteration))\n",
    "    \n",
    "    \n",
    "    if plot:\n",
    "        # Plotting Importances\n",
    "        fig, ax = plt.subplots(figsize=(24, 24))\n",
    "        xgb.plot_importance(model, height=0.4, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Model\n",
      "[03:28:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.58838\tvalid-mlogloss:1.58974\n",
      "[50]\ttrain-mlogloss:0.91360\tvalid-mlogloss:0.95750\n",
      "[100]\ttrain-mlogloss:0.58307\tvalid-mlogloss:0.65255\n",
      "[150]\ttrain-mlogloss:0.39094\tvalid-mlogloss:0.48186\n",
      "[200]\ttrain-mlogloss:0.27371\tvalid-mlogloss:0.37873\n",
      "[250]\ttrain-mlogloss:0.20020\tvalid-mlogloss:0.31611\n",
      "[300]\ttrain-mlogloss:0.15344\tvalid-mlogloss:0.27949\n",
      "[350]\ttrain-mlogloss:0.12128\tvalid-mlogloss:0.26019\n",
      "[400]\ttrain-mlogloss:0.09965\tvalid-mlogloss:0.24827\n",
      "[450]\ttrain-mlogloss:0.08434\tvalid-mlogloss:0.24140\n",
      "[500]\ttrain-mlogloss:0.07264\tvalid-mlogloss:0.23793\n",
      "[550]\ttrain-mlogloss:0.06386\tvalid-mlogloss:0.23575\n",
      "[600]\ttrain-mlogloss:0.05717\tvalid-mlogloss:0.23320\n",
      "[650]\ttrain-mlogloss:0.05208\tvalid-mlogloss:0.23215\n",
      "[700]\ttrain-mlogloss:0.04817\tvalid-mlogloss:0.23027\n",
      "[750]\ttrain-mlogloss:0.04530\tvalid-mlogloss:0.22962\n",
      "[800]\ttrain-mlogloss:0.04257\tvalid-mlogloss:0.22954\n",
      "[850]\ttrain-mlogloss:0.04023\tvalid-mlogloss:0.23116\n",
      "[875]\ttrain-mlogloss:0.03905\tvalid-mlogloss:0.23191\n",
      "Best Iteration :: 775 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1728 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_model(train, features, target, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = xgb.XGBClassifier(\n",
    "    booster='gbtree',\n",
    "    objective='multi:softprob',\n",
    "    learning_rate= 0.1,\n",
    "    num_round= 775,\n",
    "    max_depth=8,\n",
    "    seed=25,\n",
    "    nthread=-1,\n",
    "    eval_metric='mlogloss',\n",
    "    num_class=5\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, validX, trainY, validY = train_test_split(train[features], \n",
    "                                                  train[target], test_size=0.2,stratify=train[target], random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:28:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:28:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:28:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:28:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:28:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:28:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:28:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1449649336214568"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb1\n",
    "cross_valid(model,train,features,target,cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:29:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "Validation Score: 0.1377353904146378\n"
     ]
    }
   ],
   "source": [
    "model = xgb1\n",
    "model.fit(trainX[features],trainY)\n",
    "y_pred_valid = model.predict_proba(validX[features])\n",
    "print(\"Validation Score:\",metric(validY,y_pred_valid))\n",
    "y_pred_test = model.predict_proba(test[features])\n",
    "result = pd.DataFrame(y_pred_test)\n",
    "#result.to_excel(\"xgb_boost_solution1.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.998005</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.056769</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.939599</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.998748</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.026915</td>\n",
       "      <td>0.967628</td>\n",
       "      <td>0.002460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.991594</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.000498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  0.000373  0.000794  0.998005  0.000353  0.000475\n",
       "1  0.000408  0.056769  0.002893  0.939599  0.000332\n",
       "2  0.000346  0.000336  0.998748  0.000251  0.000319\n",
       "3  0.001537  0.001461  0.026915  0.967628  0.002460\n",
       "4  0.000540  0.000524  0.991594  0.006845  0.000498"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Public score\n",
    "## 0.16467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapping test features values to train feature values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.837812</td>\n",
       "      <td>-0.273636</td>\n",
       "      <td>1.276580</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>-0.585142</td>\n",
       "      <td>-0.24287</td>\n",
       "      <td>0.349804</td>\n",
       "      <td>0.12356</td>\n",
       "      <td>0.166795</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.445195</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>-2.139737</td>\n",
       "      <td>-2.527625</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.285133</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.197642</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.886135</td>\n",
       "      <td>-0.568935</td>\n",
       "      <td>1.100428</td>\n",
       "      <td>-0.244589</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>-0.217109</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.078087</td>\n",
       "      <td>-0.273636</td>\n",
       "      <td>-0.496119</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>-2.438092</td>\n",
       "      <td>-0.24287</td>\n",
       "      <td>0.349804</td>\n",
       "      <td>0.12356</td>\n",
       "      <td>0.166795</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.445195</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.513736</td>\n",
       "      <td>0.395628</td>\n",
       "      <td>0.17609</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.285133</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>-5.059644</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.27735</td>\n",
       "      <td>0.886135</td>\n",
       "      <td>0.504299</td>\n",
       "      <td>-0.434268</td>\n",
       "      <td>-0.244040</td>\n",
       "      <td>0.229718</td>\n",
       "      <td>-0.217109</td>\n",
       "      <td>0.087039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0        f1        f2        f3        f4       f5        f6  \\\n",
       "0 -0.837812 -0.273636  1.276580  0.463262 -0.585142 -0.24287  0.349804   \n",
       "1  2.078087 -0.273636 -0.496119  0.463262 -2.438092 -0.24287  0.349804   \n",
       "\n",
       "        f7        f8       f9       f10      f11       f12       f13      f14  \\\n",
       "0  0.12356  0.166795  0.06143  0.445195  0.27735 -2.139737 -2.527625  0.17609   \n",
       "1  0.12356  0.166795  0.06143  0.445195  0.27735  0.513736  0.395628  0.17609   \n",
       "\n",
       "       f15       f16      f17       f18      f19      f20       f21       f22  \\\n",
       "0  0.06143  0.285133  0.06143  0.197642  0.06143  0.27735  0.886135 -0.568935   \n",
       "1  0.06143  0.285133  0.06143 -5.059644  0.06143  0.27735  0.886135  0.504299   \n",
       "\n",
       "        f23       f24       f25       f26       f27  \n",
       "0  1.100428 -0.244589  0.229718 -0.217109  0.087039  \n",
       "1 -0.434268 -0.244040  0.229718 -0.217109  0.087039  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trick Part** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_f0 = pd.DataFrame(train['f0'].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "te_f0 = pd.DataFrame(test['f0'].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "d = pd.concat([tr_f0,te_f0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.837812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.379487</td>\n",
       "      <td>-0.421255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066123</td>\n",
       "      <td>-0.004698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511733</td>\n",
       "      <td>0.411859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.957343</td>\n",
       "      <td>0.828416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.402954</td>\n",
       "      <td>1.244973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.848564</td>\n",
       "      <td>1.661530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.294174</td>\n",
       "      <td>2.078087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         0\n",
       "0 -0.825098 -0.837812\n",
       "1 -0.379487 -0.421255\n",
       "2  0.066123 -0.004698\n",
       "3  0.511733  0.411859\n",
       "4  0.957343  0.828416\n",
       "5  1.402954  1.244973\n",
       "6  1.848564  1.661530\n",
       "7  2.294174  2.078087"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On observation, we can see there is some relation between train and test unique values.  \n",
    "The relation is found using the below equation:  \n",
    "y = ax + c**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_value = a1*test_value + c1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the a1 and c1 values for each feature.\n",
    "def calculate_transform(i,train,test):\n",
    "    tr = pd.DataFrame(train[i].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "    te = pd.DataFrame(test[i].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "\n",
    "    a1 = (tr[0][1]-tr[0][0])/(te[0][1]-te[0][0])\n",
    "    c1 = (tr[0][0]) - (te[0][0])*a1\n",
    "    #a1*te[0] + c1\n",
    "    return [a1,c1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(set(features)-set(['f1','f22','f23'])):\n",
    "    l = calculate_transform(i,train,test)\n",
    "    test[i] = l[0]*test[i]+l[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f23,f1,f22 features have unequal number of unique values so they r calculated separetely**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1\n",
    "tr_f0 = pd.DataFrame(train['f1'].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "te_f0 = pd.DataFrame(test['f1'].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "a1 = (tr_f0[0][9]-tr_f0[0][8])/(te_f0[0][6]-te_f0[0][5])\n",
    "c1 = (tr_f0[0][8]) - (te_f0[0][5])*a1\n",
    "test['f1'] = a1*test['f1'] + c1\n",
    "\n",
    "#f22\n",
    "tr_f0 = pd.DataFrame(train['f22'].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "te_f0 = pd.DataFrame(test['f22'].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "a1 = (tr_f0[0][48]-tr_f0[0][47])/(te_f0[0][36]-te_f0[0][35])\n",
    "c1 = (tr_f0[0][47]) - (te_f0[0][35])*a1\n",
    "test['f22'] = a1*test['f22'] + c1\n",
    "\n",
    "\n",
    "#f23\n",
    "tr_f0 = pd.DataFrame(train['f23'].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "te_f0 = pd.DataFrame(test['f23'].unique()).sort_values(by=0).reset_index(drop=True)\n",
    "a1 = (tr_f0[0][62]-tr_f0[0][61])/(te_f0[0][40]-te_f0[0][39])\n",
    "c1 = (tr_f0[0][61]) - (te_f0[0][39])*a1\n",
    "test['f23'] = a1*test['f23'] + c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>1.388246</td>\n",
       "      <td>0.4094</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>-1.999287</td>\n",
       "      <td>-2.118189</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>-8.750026</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>-0.573268</td>\n",
       "      <td>1.087230</td>\n",
       "      <td>-0.287622</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>-7.812837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.294174</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.4094</td>\n",
       "      <td>-2.356907</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.548623</td>\n",
       "      <td>0.472101</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>-5.477226</td>\n",
       "      <td>-8.750026</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>0.925358</td>\n",
       "      <td>0.443257</td>\n",
       "      <td>-0.406121</td>\n",
       "      <td>-0.287096</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>-7.812837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>1.388246</td>\n",
       "      <td>0.4094</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.548623</td>\n",
       "      <td>0.472101</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>-8.750026</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>-0.573268</td>\n",
       "      <td>-0.406121</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>-7.812837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.4094</td>\n",
       "      <td>1.305455</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>-2.695676</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.548623</td>\n",
       "      <td>0.472101</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>-5.477226</td>\n",
       "      <td>-8.750026</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>-0.460446</td>\n",
       "      <td>-1.850510</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>-7.812837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.825098</td>\n",
       "      <td>-0.26425</td>\n",
       "      <td>-0.461423</td>\n",
       "      <td>0.4094</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.276144</td>\n",
       "      <td>-2.695676</td>\n",
       "      <td>0.090167</td>\n",
       "      <td>0.107958</td>\n",
       "      <td>0.06143</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.548623</td>\n",
       "      <td>0.472101</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.308879</td>\n",
       "      <td>0.040193</td>\n",
       "      <td>0.182574</td>\n",
       "      <td>-8.750026</td>\n",
       "      <td>0.233285</td>\n",
       "      <td>-1.080663</td>\n",
       "      <td>-0.573268</td>\n",
       "      <td>-0.406121</td>\n",
       "      <td>-0.687687</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>-0.232472</td>\n",
       "      <td>-7.812837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f0       f1        f2      f3        f4        f5        f6  \\\n",
       "0 -0.825098 -0.26425  1.388246  0.4094 -0.525726 -0.276144  0.370965   \n",
       "1  2.294174 -0.26425 -0.461423  0.4094 -2.356907 -0.276144  0.370965   \n",
       "2 -0.825098 -0.26425  1.388246  0.4094 -0.525726 -0.276144  0.370965   \n",
       "3 -0.825098 -0.26425 -0.461423  0.4094  1.305455 -0.276144 -2.695676   \n",
       "4 -0.825098 -0.26425 -0.461423  0.4094 -0.525726 -0.276144 -2.695676   \n",
       "\n",
       "         f7        f8       f9       f10       f11       f12       f13  \\\n",
       "0  0.090167  0.107958  0.06143  0.395874  0.308879 -1.999287 -2.118189   \n",
       "1  0.090167  0.107958  0.06143  0.395874  0.308879  0.548623  0.472101   \n",
       "2  0.090167  0.107958  0.06143  0.395874  0.308879  0.548623  0.472101   \n",
       "3  0.090167  0.107958  0.06143  0.395874  0.308879  0.548623  0.472101   \n",
       "4  0.090167  0.107958  0.06143  0.395874  0.308879  0.548623  0.472101   \n",
       "\n",
       "        f14       f15       f16       f17       f18       f19       f20  \\\n",
       "0  0.172917  0.098853  0.308879  0.040193  0.182574 -8.750026  0.233285   \n",
       "1  0.172917  0.098853  0.308879  0.040193 -5.477226 -8.750026  0.233285   \n",
       "2  0.172917  0.098853  0.308879  0.040193  0.182574 -8.750026  0.233285   \n",
       "3  0.172917  0.098853  0.308879  0.040193 -5.477226 -8.750026  0.233285   \n",
       "4  0.172917  0.098853  0.308879  0.040193  0.182574 -8.750026  0.233285   \n",
       "\n",
       "        f21       f22       f23       f24       f25       f26       f27  \n",
       "0  0.925358 -0.573268  1.087230 -0.287622  0.271886 -0.232472 -7.812837  \n",
       "1  0.925358  0.443257 -0.406121 -0.287096  0.271886 -0.232472 -7.812837  \n",
       "2 -1.080663 -0.573268 -0.406121 -0.687687  0.271886 -0.232472 -7.812837  \n",
       "3 -1.080663 -0.460446 -1.850510 -0.687687  0.271886 -0.232472 -7.812837  \n",
       "4 -1.080663 -0.573268 -0.406121 -0.687687  0.271886 -0.232472 -7.812837  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_model(train,features,target,plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = xgb.XGBClassifier(\n",
    "    booster='gbtree',\n",
    "    objective='multi:softprob',\n",
    "    learning_rate= 0.1,\n",
    "    num_round= 775,\n",
    "    max_depth=8,\n",
    "    seed=25,\n",
    "    nthread=-1,\n",
    "    eval_metric='mlogloss',\n",
    "    num_class=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:29:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[03:29:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1449649336214568"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb1\n",
    "cross_valid(model,train,features,target,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, validX, trainY, validY = train_test_split(train[features], \n",
    "                                                  train[target], test_size=0.2,stratify=train[target], random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:29:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "Validation Score: 0.1377353904146378\n"
     ]
    }
   ],
   "source": [
    "model = xgb1\n",
    "model.fit(trainX,trainY)\n",
    "y_pred_valid = model.predict_proba(validX)\n",
    "print(\"Validation Score:\",metric(validY,y_pred_valid))\n",
    "y_pred_test = model.predict_proba(test[features])\n",
    "result = pd.DataFrame(y_pred_test)\n",
    "#result.to_excel(\"xgb_boost_trick_part.xlsx\",index=False)\n",
    "##0.08903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.998291</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.984513</td>\n",
       "      <td>0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.998772</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.046509</td>\n",
       "      <td>0.948377</td>\n",
       "      <td>0.002057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.996106</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  0.000357  0.000561  0.998291  0.000337  0.000454\n",
       "1  0.000418  0.011266  0.003463  0.984513  0.000340\n",
       "2  0.000331  0.000363  0.998772  0.000227  0.000306\n",
       "3  0.001625  0.001432  0.046509  0.948377  0.002057\n",
       "4  0.000504  0.000554  0.996106  0.002369  0.000467"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final submission which got highest private score and 0.08903 public score\n",
    "#result.to_excel(\"xgb_boost_trick_part.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
